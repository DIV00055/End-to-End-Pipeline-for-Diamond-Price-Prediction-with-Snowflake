{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "ebvywisl27atg6kpq7lu",
   "authorId": "5212056138416",
   "authorName": "DIVYANSHSEN",
   "authorEmail": "sen00055@umn.edu",
   "sessionId": "111ce971-f843-4f40-a09f-b6179fa88e13",
   "lastEditTime": 1745536218588
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2",
    "codeCollapsed": false
   },
   "source": "-- Welcome to Snowflake Notebooks!\n-- Try out a SQL cell to generate some data.\nSELECT 'FRIDAY' as SNOWDAY, 0.2 as CHANCE_OF_SNOW\nUNION ALL\nSELECT 'SATURDAY',0.5\nUNION ALL \nSELECT 'SUNDAY', 0.9;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "source": "# Then, we can use the python name to turn cell2 into a Pandas dataframe\nmy_df = cell2.to_pandas()\n\n# Chart the data\nst.subheader(\"Chance of SNOW â„ï¸\")\nst.line_chart(my_df, x='SNOWDAY', y='CHANCE_OF_SNOW')\n\n# Give it a go!\nst.subheader(\"Try it out yourself and show off your skills ðŸ¥‡\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ca7c19af-f7bb-4962-acbc-92eefed3cec7",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "from sklearn.preprocessing import OrdinalEncoder",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7db4366-463c-4faa-8fc9-4f457584bb2b",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcf13a17-9411-4991-b5de-a34f23bdedff",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "price_query = session.sql(\"\"\"\n    SELECT * \n    FROM \"DIAMONDS_DB\".\"DIAMONDPRICES_SCHEMA\".\"PRICE_DETAILS\"\n    LIMIT 1000\n\"\"\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31fec4a8-f0cb-4c8f-bbbe-ff74c845d63e",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "df = price_query.to_pandas()\ndf.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "789e171c-e91d-443b-b483-71637d37c7a1",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "df = df.drop(columns=[\"INDEX\"])\n\ndf = df.reset_index(drop=True)\n\ndf.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "181bcb87-c903-4f09-be7c-c55d79f268a4",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "import numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b45785f-ec33-4411-84ba-27c7a00d39b0",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "037ed25e-88ba-4348-a985-5f1e8ffa0d22",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport pandas as pd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9aeaf844-231b-4a33-965a-fdf24be92566",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "# 1. Feature Engineering\ndf[\"CARAT_SQ\"] = df[\"CARAT\"] ** 2\ndf[\"VOLUME\"] = df[\"X\"] * df[\"Y\"] * df[\"Z\"]\n\n# 2. Remove invalid volume rows\ndf = df[(df[\"VOLUME\"] > 0.1) & (df[\"X\"] > 0.1) & (df[\"Y\"] > 0.1) & (df[\"Z\"] > 0.1)]\n\n# 3. Encode categorical features\nencoder = OrdinalEncoder()\ndf[[\"CUT\", \"COLOR\", \"CLARITY\"]] = encoder.fit_transform(df[[\"CUT\", \"COLOR\", \"CLARITY\"]])\n\n# 4. Remove top 1% PRICE outliers\nq99 = df[\"PRICE\"].quantile(0.99)\ndf = df[df[\"PRICE\"] < q99]\n\n# 5. Select features and target\nfeatures = [\n    \"CARAT\", \"CUT\", \"COLOR\", \"CLARITY\", \"DEPTH\", \"TABLES\",\n    \"CARAT_SQ\", \"VOLUME\", \"X\", \"Y\", \"Z\"\n]\nX = df[features]\ny = df[\"PRICE\"]\n\n# 6. Train/test split and log-transform target\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_train_log = np.log1p(y_train)\ny_test_log = np.log1p(y_test)\n\n\n# 7. Normalize numeric features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# 8. Train models on scaled data\nrf_model = RandomForestRegressor(\n    n_estimators=250, max_depth=12, min_samples_split=4,\n    max_features='sqrt', random_state=42, n_jobs=1\n)\nlgbm_model = LGBMRegressor(\n    n_estimators=300, learning_rate=0.05, num_leaves=31,\n    max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=1\n)\n\nrf_model.fit(X_train_scaled, y_train_log)\nlgbm_model.fit(X_train_scaled, y_train_log)\n\n# 9. Predict and reverse log transformation\nrf_pred = np.expm1(rf_model.predict(X_test_scaled))\nlgbm_pred = np.expm1(lgbm_model.predict(X_test_scaled))\ny_true = np.expm1(y_test_log)\n\n# 10. Evaluate\nresults = []\nfor name, pred in zip([\"Random Forest\", \"LightGBM\"], [rf_pred, lgbm_pred]):\n    rmse = mean_squared_error(y_true, pred, squared=False)\n    r2 = r2_score(y_true, pred)\n    results.append({\"Model\": name, \"RMSE\": round(rmse, 2), \"RÂ² Score\": round(r2, 4)})\n\nresults_df = pd.DataFrame(results).sort_values(by=\"RMSE\")\nprint(\"Model Evaluation Results (with normalization):\")\nprint(results_df)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "363f4c85-0b23-493a-8528-dd5c0dfcc2d8",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "#train on the features and will evaluate on next step\n\n# 1. Feature Engineering\ndf[\"CARAT_SQ\"] = df[\"CARAT\"] ** 2\ndf[\"VOLUME\"] = df[\"X\"] * df[\"Y\"] * df[\"Z\"]\n\n# 2. Remove invalid volume rows\ndf = df[(df[\"VOLUME\"] > 0.1) & (df[\"X\"] > 0.1) & (df[\"Y\"] > 0.1) & (df[\"Z\"] > 0.1)]\n\n# 3. Encode categoricals\nencoder = OrdinalEncoder()\ndf[[\"CUT\", \"COLOR\", \"CLARITY\"]] = encoder.fit_transform(df[[\"CUT\", \"COLOR\", \"CLARITY\"]])\n\n# 4. Remove top 1% PRICE outliers\nq99 = df[\"PRICE\"].quantile(0.99)\ndf = df[df[\"PRICE\"] < q99]\n\n# 5. Select features and log-transformed target\nfeatures = [\n    \"CARAT\", \"CUT\", \"COLOR\", \"CLARITY\", \"DEPTH\", \"TABLES\",\n    \"CARAT_SQ\", \"VOLUME\", \"X\", \"Y\", \"Z\"\n]\nX = df[features]\ny = df[\"PRICE\"]\ny_log = np.log1p(y)  # Log-transform full target\n\n# 6. Normalize full dataset (no split)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 7. Train models on entire dataset\nrf_model = RandomForestRegressor(\n    n_estimators=250, max_depth=12, min_samples_split=4,\n    max_features='sqrt', random_state=42, n_jobs=1\n)\nlgbm_model = LGBMRegressor(\n    n_estimators=300, learning_rate=0.05, num_leaves=31,\n    max_depth=6, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=1\n)\n\nrf_model.fit(X_scaled, y_log)\nlgbm_model.fit(X_scaled, y_log)\n\nprint(\"Models trained successfully on full dataset (log-transformed + normalized).\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d169f9d-4adc-4547-84d1-5ade67272173",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "# Predict using trained models\nrf_pred_log = rf_model.predict(X_scaled)\nlgbm_pred_log = lgbm_model.predict(X_scaled)\n\n# Reverse log transform predictions and target\nrf_pred = np.expm1(rf_pred_log)\nlgbm_pred = np.expm1(lgbm_pred_log)\ny_actual = np.expm1(y_log)\n\n# Evaluate model performance\nresults = []\nfor name, pred in zip([\"Random Forest\", \"LightGBM\"], [rf_pred, lgbm_pred]):\n    rmse = mean_squared_error(y_actual, pred, squared=False)\n    r2 = r2_score(y_actual, pred)\n    results.append({\n        \"Model\": name,\n        \"RMSE\": round(rmse, 2),\n        \"RÂ² Score\": round(r2, 4)\n    })\n\n# Display results\nresults_df = pd.DataFrame(results).sort_values(by=\"RMSE\")\nprint(\"Final Evaluation Results:\\n\")\nprint(results_df.to_string(index=False))\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b8c00cd2-d6f4-4e69-be6c-daf2625dcf27",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "rf_pred_log = rf_model.predict(X_scaled)\nlgbm_pred_log = lgbm_model.predict(X_scaled)\n\nrf_pred = np.expm1(rf_pred_log)\nlgbm_pred = np.expm1(lgbm_pred_log)\ny_actual = np.expm1(y_log)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df519ca8-0347-4791-a339-ecc0438a4230",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "# 1. Actual vs Predicted Scatter Plot for both models\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=y_actual, y=rf_pred, alpha=0.5, label=\"Random Forest\", color=\"steelblue\")\nsns.scatterplot(x=y_actual, y=lgbm_pred, alpha=0.5, label=\"LightGBM\", color=\"darkorange\")\nsns.lineplot(x=y_actual, y=y_actual, color=\"black\", linestyle=\"--\", label=\"Ideal Fit\")\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Actual vs Predicted Prices (Random Forest vs LightGBM)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 2. Residual Plot Comparison\nrf_residuals = y_actual - rf_pred\nlgbm_residuals = y_actual - lgbm_pred\n\nplt.figure(figsize=(10, 5))\nsns.kdeplot(rf_residuals, label=\"Random Forest\", fill=True, color=\"steelblue\")\nsns.kdeplot(lgbm_residuals, label=\"LightGBM\", fill=True, color=\"darkorange\")\nplt.axvline(0, color=\"black\", linestyle=\"--\")\nplt.title(\"Residual Distribution Comparison\")\nplt.xlabel(\"Prediction Error\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 3. Side-by-side Feature Importances\nrf_importances = rf_model.feature_importances_\nlgbm_importances = lgbm_model.feature_importances_\n\nfeature_df = pd.DataFrame({\n    \"Feature\": X.columns,\n    \"Random Forest\": rf_importances,\n    \"LightGBM\": lgbm_importances\n}).set_index(\"Feature\")\n\nfeature_df.plot(kind=\"barh\", figsize=(10, 6), color=[\"steelblue\", \"darkorange\"])\nplt.title(\"Feature Importances (Random Forest vs LightGBM)\")\nplt.xlabel(\"Importance Score\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b924a592-0e09-4678-a284-86c6617056c3",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}